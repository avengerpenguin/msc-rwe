\chapter{Background}
\label{intro}

This report proposes improvements to the BBC Search product making use
of \emph{Linked Data}\cite{bizer2009linked} and/or the Semantic
Web\cite{berners2001semantic}. I hope to be able to demonstrate five tangible
benefits this domain can bring to a search application, namely:

\begin{enumerate}
  \item Further decoupling of BBC applications, which aids scalability,
    maintainability and resilience in many software architectures.\cite{}
  \item Improvements to the search results page that help users determine
    if the results given are relevant via so-called
    \emph{Rich Snippets}\cite{goel2009introducing}
    that can automatically display some semantic information about a given
    result without the need for manual curation.
  \item User experience and design improvements to the search results
    where semantic properties about the results can drive visual decoration
    and calls to action, which can allow for a more flexible and
    adaptable design without the need to enumerate all ``kinds'' with
    different templates.
  \item Removal of the manual curation of the ``Editor's Choice'' results
    against a given query and replacing it with curated linked data that
    can provide recommended content against queries in a more general
    manner.
  \item Replacement of the manual curation of autocomplete suggestions
    in the \emph{Search Suggest} service -- that suggests full text queries
    as the user types -- with suggestions driven from linked data.
\end{enumerate}

In the rest of this chapter, I will give an overview of what role BBC
Search plays and how we can usefully view the system as four
distinct subsystems. In later chapters we can then look to improve these
four separate systems one at a time and then evaluate how these improvements
might complement each other.

\section{BBC Search}

BBC Search is an online product that forms part of the BBC website
to enable users to search for and navigate all areas of the site using
free text queries. This might aid visitors who are unable to find their desired
content via other global or site-specific navigation or those who are simply
looking for things that fall below the radar of the latest, greatest or
otherwise promoted content.

General-purpose search engines such as Google are also capable of directing
users to BBC pages via free text queries, but there is a benefit the BBC
can bring with its own search service that has greater knowledge of the
BBC's own content. The BBC can use internal knowledge to predict a user's
intent (e.g. if the query matches a programme title, then show links to watch
the latest episode on iPlayer) or to provide editorially-curated
links to significant pages (e.g. if a user searches for a term
related to a major, ongoing news theme, curated links can
link to an overview or FAQ for that ongoing event for those
looking for the background on it).

It is well established that a classical information retrieval system
can be expressed as two sets of computation: processing of information
into indexes (also known as \emph{indexing}) for quicker retrieval
and the retrieval of documents from those indexes based on a user's
query. These can be known as \emph{index time} or \emph{query time}
processing respectively.

The general strategy is to perform
any expensive transformation, expansion or ordering of data
at index time so that query time algorithms may be optimised to
run as quickly as possible. This is an approach that has only
increased in importance as information retrieval systems have moved
from standalone systems (e.g. a catalogue search situated within
a library used by a handful of users at once) to web-based
searches across a far larger corpus with tens or even hundreds of
users submitting queries in a single second.

In the remaining sections of this chapter, I outline the considerations
around these two sides of the search application in addition to the
two complementary systems known as \emph{Editor's Choice} and
\emph{Search Suggest}.

\section{Indexing Content}

% TODO

\section{Retrieving Content}

Once we have constructed an index optimised for keyword-based retrieval,
we can begin to run queries against that index. A retrieval problem
can be summarised as trying to maximise two separate measures: \emph{precision}
and \emph{recall}, where:

\begin{displaymath}
  \textbf{precision} = \frac{
    |\text{relevant documents} \cap \text{retrieved documents}|
  }{
    |\text{retrieved documents}|
  }
\end{displaymath}

\begin{displaymath}
  \textbf{recall} = \frac{
    |{relevant documents} \cap \text{retrieved documents}|
  }{
    |\text{relevant documents}|
  }
\end{displaymath}

Namely, precision is the fraction of the retrieved documents that were
relevant to the user's query and recall is the fraction of documents out of
all possible, relevant documents that appear in the search results.

Note that a system could trivially maximise recall by always returning all
results, but such a system would perform poorly in terms of precision. A
measure known as \emph{F-measure} was derived by van Rijsbergen
\cite{rijsbergen1979information} which
balances precision recall based on preferring recall over precision $\beta$
times:

\begin{displaymath}
  F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{precision} \cdot \mathrm{recall} }{ \beta^2 \cdot \mathrm{precision} + \mathrm{recall}}
\end{displaymath}

There are other measures such as average precision and average mean precision
that take into account ordering of results and the average over multiple
queries respectively. A successful algorithm or third party search engine
application can be evaluated quantitatively by these measures and more. A more
qualitative summary from the perspective of the end users could be:

\begin{quote}
  A successful search application will return only the content the user wants
  (precision) --
  as high up the results list as possible -- and as much of desired content
  as possible (recall).
\end{quote}

The set of all documents that could be retrieved by a query is known as the
\emph{corpus} and search systems will typically apply a \emph{score} against
each that gives some measure of confidence of the relevance of each document
given a particular query. It should be noted that scores are a relative
metric only and only have meaning within the context of a particular
retrieval algorithm and particular query.

\section{Editor's Choice}

The current BBC Search application returns an ordered list of results
for queries based on keyword matching within the titles, text or other
chosen metadata fields as appropriate. However, there are cases where
the algorithms employed may not return something noted as the official
or canonical match for a concept, person, programme, etc. (e.g. the
official microsite for a popular BBC programme or a profile page for
a notable person on BBC News).

Editorial staff thus have control over a curation system that manifests
as an \emph{Editor's Choice} feature that will ensure the top one to three
results for popular queries link to manually-chosen pages deemed as the most
relevant pages for those queries. For instance, a search for a city such
as \emph{Coventry} might naturally return recent news articles that mention the
city (since news results are biased towards more recent articles over older
ones), but the Editor's Choice links on top of the so-called \emph{organic
results} can persistently offer links for common use cases such as
the BBC Weather page for Coventry.

Each and every link promoted within this feature is hand-chosen by editorial
staff and much manual effort is needed to keep promoting new concepts
as they appear and to remove pages as they get superseded. It is hoped
in this proposal we can reduce some of that manual curation through the use
of linked data and demonstrate ways editorial staff can focus on curating
their own linked data such that Editor's Choice could evolve into a more
generalised and automated subsystem. A metadata-driven curation system
could use common patterns in the data to promote orders of magnitude more
top links with a fraction of the current effort.

\section{Search Suggest}

One challenge for the querying of a search application is to be tolerant
of the variable quality of user-entered queries. Many users of web search
engines may give ambiguously short queries, overly long queries with (where
it can be hard for a machine to pick out the important keywords amongst
less important words or stop words\cite{rajaraman2011data})
or simply use incorrect spelling.

This problem can be ameliorated with a plethora of algorithm
server-side solutions (e.g. spell checking), but a simple solution that
can be implemented on the user interface is an \emph{autocomplete}
or \emph{autosuggest} feature. The former prompts for potential
queries, titles, names, etc. that they may be typing so as to prevent
the need to type the query out in full. The latter feature can go
further to suggest onwards journeys that bypass the need for a
page of search results altogether.

The search box for BBC Search implements an autocomplete
feature -- known
as \emph{Search Suggest} -- whereby
the full names or titles of are suggested to users as they type the first
few characters of their queries. This helps guide users into free text
queries that are more likely to match terms in the indexes and reduces
mismatching due to misspellings or otherwise poorly-formed queries.

The list of suggestions that might surface in this system are
currently chosen and maintained by hand by editorial staff. If a new
concept breaks into the news (such as a notable person becoming notable
for the first time), then a person must enter that term into the system.
Similarly, if something should not be suggested to users any longer
(perhaps for legal reasons or simply because it is no longer a relevant
concept), then it must be amended or removed manually.

This manual maintenance of the suggestion list also leads to a
disconnect between searchable content and suggested queries that might
appear as users type. There is an onus on editorial staff to ensure
that content will be found when a suggested query is used. It might
serve the website users better to find mechanisms by which to generate
a useful set of suggestions from the corpus of content as it is
indexed. The heterogenous nature of BBC content turns this into
further integration work where different business rules are likely required
to generate suggestions from different types of content.

In chapter~\ref{stateoftheart} I will briefly outline some of the state of the
art in the Linked Data domain and start to highlight areas that show
the most promise for improving the four areas of the search application
described in this chapter.
