\documentclass[10pt,a4paper]{report}

\usepackage{mathtools}

\title{Proposal to improve BBC Search through the use of Linked Data}
\author{Ross Fenning}

\begin{document}

\maketitle

\chapter{Background}

This report proposes improvements to the BBC Search product making use
of \emph{Linked Data}\cite{bizer2009linked} and/or the Semantic
Web\cite{berners2001semantic}. I hope to be able to demonstrate five tangible
benefits this domain can bring to a search application, namely:

\begin{enumerate}
  \item Further decoupling of BBC applications, which aids scalability,
    maintainability and resilience in many software architectures.\cite{}
  \item Improvements to the search results page that help users determine
    if the results given are relevant via so-called
    \emph{Rich Snippets}\cite{goel2009introducing}
    that can automatically display some semantic information about a given
    result without the need for manual curation.
  \item User experience and design improvements to the search results
    where semantic properties about the results can drive visual decoration
    and calls to action, which can allow for a more flexible and
    adaptable design without the need to enumerate all ``kinds'' with
    different templates.
  \item Removal of the manual curation of the ``Editor's Choice'' results
    against a given query and replacing it with curated linked data that
    can provide recommended content against queries in a more general
    manner.
  \item Replacement of the manual curation of autocomplete suggestions
    in the \emph{Search Suggest} service -- that suggests full text queries
    as the user types -- with suggestions driven from linked data.
\end{enumerate}

In the rest of this chapter, I will give an overview of what role BBC
Search plays and how we can usefully view the system as four
distinct subsystems. In later chapters we can then look to improve these
four separate systems one at a time and then evaluate how these improvements
might complement each other.

\section{BBC Search}

BBC Search is an online product that forms part of the BBC website
to enable users to search for and navigate all areas of the site using
free text queries. This might aid visitors who are unable to find their desired
content via other global or site-specific navigation or those who are simply
looking for things that fall below the radar of the latest, greatest or
otherwise promoted content.

General-purpose search engines such as Google are also capable of directing
users to BBC pages via free text queries, but there is a benefit the BBC
can bring with its own search service that has greater knowledge of the
BBC's own content. The BBC can use internal knowledge to predict a user's
intent (e.g. if the query matches a programme title, then show links to watch
the latest episode on iPlayer) or to provide editorially-curated
links to significant pages (e.g. if a user searches for a term
related to a major, ongoing news theme, curated links can
link to an overview or FAQ for that ongoing event for those
looking for the background on it).

It is well established that a classical information retrieval system
can be expressed as two sets of computation: processing of information
into indexes (also known as \emph{indexing}) for quicker retrieval
and the retrieval of documents from those indexes based on a user's
query. These can be known as \emph{index time} or \emph{query time}
processing respectively.

The general strategy is to perform
any expensive transformation, expansion or ordering of data
at index time so that query time algorithms may be optimised to
run as quickly as possible. This is an approach that has only
increased in importance as information retrieval systems have moved
from standalone systems (e.g. a catalogue search situated within
a library used by a handful of users at once) to web-based
searches across a far larger corpus with tens or even hundreds of
users submitting queries in a single second.

In the remaining sections of this chapter, I outline the considerations
around these two sides of the search application in addition to the
two complementary systems known as \emph{Editor's Choice} and
\emph{Search Suggest}.

\section{Indexing Content}

The Search application needs to read in or receive metadata about
BBC content from a variety of sources and then needs to serve
a diverse set of use cases.\cite{fenning2014applicability} The
need to integrate against different data sources and funnel
that information into a common set of search indexes can
typically be seen as an \emph{enterprise integration}
problem. Architectural patterns have been developed, tested and
implemented in a large number of enterprises to synchronise,
transform and normalise data. Such patterns are notably
summarised by Hohpe and Woolf.\cite{hohpe2004enterprise}

Some of the data is fairly flat, textual content such as articles and some
information -- such as programmes -- is very structured (e.g. programmes,
series, episodes, times of broadcast, channel on which they are broadcast,
what times they are available to watch on iPlayer, what kinds of devices
are permitted by rights to watch them on iPlayer). There are challenges
in providing a consistent and meaningful user experience across such
a heterogeneous mix of content.

We can achieve some uniformity of the data by building a single, common
API through which all content is fed into the Search application. However,
some integration work needs to be
done by either the owners of the content to send content to the API
or the developers of the Search application, who can integrate
existing feeds via an adapter pattern.

Some pages on the BBC website no
longer have a dedicated team maintaining them and can sometimes only
be discovered via a web crawl as an external search engine would do. A lot
of metadata about content also lives in a diverse range of databases and
content management systems, which can be collectively described as part
of the so-called \emph{deep web}.\cite{}

In chapter~\ref{stateoftheart}, we will discuss how the deep web is one
problem the linked data domain seeks to solve.

\section{Retrieving Content}

Once we have constructed an index optimised for keyword-based retrieval,
we can begin to run queries against that index. A retrieval problem
can be summarised as trying to maximise two separate measures: \emph{precision}
and \emph{recall}, where:

\begin{displaymath}
  \textbf{precision} = \frac{
    |\text{relevant documents} \cap \text{retrieved documents}|
  }{
    |\text{retrieved documents}|
  }
\end{displaymath}

\begin{displaymath}
  \textbf{recall} = \frac{
    |{relevant documents} \cap \text{retrieved documents}|
  }{
    |\text{relevant documents}|
  }
\end{displaymath}

Namely, precision is the fraction of the retrieved documents that were
relevant to the user's query and recall is the fraction of documents out of
all possible, relevant documents that appear in the search results.

Note that a system could trivially maximise recall by always returning all
results, but such a system would perform poorly in terms of precision. A
measure known as \emph{F-measure} was derived by van Rijsbergen
\cite{rijsbergen1979information} which
balances precision recall based on preferring recall over precision $\beta$
times:

\begin{displaymath}
  F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{precision} \cdot \mathrm{recall} }{ \beta^2 \cdot \mathrm{precision} + \mathrm{recall}}
\end{displaymath}

There are other measures such as average precision and average mean precision
that take into account ordering of results and the average over multiple
queries respectively. A successful algorithm or third party search engine
application can be evaluated quantitatively by these measures and more. A more
qualitative summary from the perspective of the end users could be:

\begin{quote}
  A successful search application will return only the content the user wants
  (precision) --
  as high up the results list as possible -- and as much of desired content
  as possible (recall).
\end{quote}

The set of all documents that could be retrieved by a query is known as the
\emph{corpus} and search systems will typically apply a \emph{score} against
each that gives some measure of confidence of the relevance of each document
given a particular query. It should be noted that scores are a relative
metric only and only have meaning within the context of a particular
retrieval algorithm and particular query.

\section{Editor's Choice}

The current BBC Search application returns an ordered list of results
for queries based on keyword matching within the titles, text or other
chosen metadata fields as appropriate. However, there are cases where
the algorithms employed may not return something noted as the official
or canonical match for a concept, person, programme, etc. (e.g. the
official microsite for a popular BBC programme or a profile page for
a notable person on BBC News).

Editorial staff thus have control over a curation system that manifests
as an \emph{Editor's Choice} feature that will ensure the top one to three
results for popular queries link to manually-chosen pages deemed as the most
relevant pages for those queries. For instance, a search for a city such
as \emph{Coventry} might naturally return recent news articles that mention the
city (since news results are biased towards more recent articles over older
ones), but the Editor's Choice links on top of the so-called \emph{organic
results} can persistently offer links for common use cases such as
the BBC Weather page for Coventry.

Each and every link promoted within this feature is hand-chosen by editorial
staff and much manual effort is needed to keep promoting new concepts
as they appear and to remove pages as they get superseded. It is hoped
in this proposal we can reduce some of that manual curation through the use
of linked data and demonstrate ways editorial staff can focus on curating
their own linked data such that Editor's Choice could evolve into a more
generalised and automated subsystem. A metadata-driven curation system
could use common patterns in the data to promote orders of magnitude more
top links with a fraction of the current effort.

\section{Search Suggest}

One challenge for the querying of a search application is to be tolerant
of the variable quality of user-entered queries. Many users of web search
engines may give ambiguously short queries, overly long queries with (where
it can be hard for a machine to pick out the important keywords amongst
less important words or stop words\cite{rajaraman2011data})
or simply use incorrect spelling.

This problem can be ameliorated with a plethora of algorithm
server-side solutions (e.g. spell checking), but a simple solution that
can be implemented on the user interface is an \emph{autocomplete}
or \emph{autosuggest} feature. The former prompts for potential
queries, titles, names, etc. that they may be typing so as to prevent
the need to type the query out in full. The latter feature can go
further to suggest onwards journeys that bypass the need for a
page of search results altogether.

The search box for BBC Search implements an autocomplete
feature -- known
as \emph{Search Suggest} -- whereby
the full names or titles of are suggested to users as they type the first
few characters of their queries. This helps guide users into free text
queries that are more likely to match terms in the indexes and reduces
mismatching due to misspellings or otherwise poorly-formed queries.

The list of suggestions that might surface in this system are
currently chosen and maintained by hand by editorial staff. If a new
concept breaks into the news (such as a notable person becoming notable
for the first time), then a person must enter that term into the system.
Similarly, if something should not be suggested to users any longer
(perhaps for legal reasons or simply because it is no longer a relevant
concept), then it must be amended or removed manually.

This manual maintenance of the suggestion list also leads to a
disconnect between searchable content and suggested queries that might
appear as users type. There is an onus on editorial staff to ensure
that content will be found when a suggested query is used. It might
serve the website users better to find mechanisms by which to generate
a useful set of suggestions from the corpus of content as it is
indexed. The heterogenous nature of BBC content turns this into
further integration work where different business rules are likely required
to generate suggestions from different types of content.

In chapter~\ref{stateoftheart} I will briefly outline some of the state of the
art in the Linked Data domain and start to highlight areas that show
the most promise for improving the four areas of the search application
described in this chapter.


\chapter{Linked Data}
\label{stateoftheart}

The concept of Linked Data can well be summarised as a representation of
data that follows four principles:\cite{berners2011linked}

\begin{enumerate}
  \item Use URIs to denote things.
  \item Prefer HTTP URIs (i.e. URLs) so that these things can be looked
    up (i.e. \emph{dereferenced}.
  \item When these URIs are dereferenced, provide useful information about
    those things using standards.
  \item Include links to other things (using their URIs) when such
    information is published on the web.
\end{enumerate}

It can be seen as a subset of Semantic Web techniques and it builds upon
the \emph{Resource Description Framework} (RDF)\cite{lassila1999resource},
which is a general-purpose method for describing and modelling information.

In this chapter, I will outline how web authors are encouraged to publish
linked data within their content and how that is already used to enhance
search results in global search engines. I will then introduce how linked
data may be used within an enterprise setting to reduce traditional
integration efforts and then touch on some other aspects of the linked
data domain that could be of interest to the search application.

\section{Publishing Linked Data within Web Pages}

Whilst there exist multiple formats for publishing RDF purely for machines
(e.g. Turtle\cite{world2014rdf}, N-Triples, JSON-LD\cite{world2014json},
N3, RDF/XML) there are multiple ways to embed semantic or linked data within
an HTML web page:

\begin{itemize}
  \item RDFa\cite{adida2012rdfa} and RDFa Lite\cite{lite2004rdfa}.
  \item DC-HTML defines how to embed the Dublin Core vocabulary within HTML
    meta tags.
\end{itemize}

\section{Enhancing Search Engine Results}

\section{Linked Enterprise Data}

In recent years, a growing trend has emerged to use
\emph{Linked Data} in the enterprise to remove the need
for repeated integration projects or data warehousing.
\cite{allemang2010semantic} If we view the indexing processes
of a search application as a variant of data warehousing,
we can start to argue for enterprise-wide publishing of
linked data as a more cost effective means of building search
indexes over bespoke integration efforts.

An important consideration is that BBC Search is largely directing
users to public pages on the BBC website, a large number of which have
already published some of their metadata as linked data. Notably, BBC News,
BBC Sport and BBC Food all use some combination
of microformats, microdata, RDFa, HTML meta tags within their HTML content
pages. Some of this was to improve search results in external search engines
such as Google and some of it ties into Facebook's Open Graph Protocol, which
enables and enhances social media sharing of the pages.

These efforts are not as structured and rich as the more notable uses
of semantic web technologies on the programmes microsites, Nature and BBC
Music \cite{raimond2010use}, but indicate that some level of metadata
can be retrieved
as linked data. In cases where the metadata are rich enough, it might be
possible to avoid direct integration with databases, hidden back end
web services and other structured ``feeds'' typically set up for enterprise
integration.

\section{Reasoning and Inference}

\chapter{Analysis}

\chapter{Proposal}

\chapter{Implementation Plan}

\chapter{Summary}

\bibliographystyle{plain}
\bibliography{bibtex}

\end{document}
