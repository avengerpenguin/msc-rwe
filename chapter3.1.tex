\section{Indexing Content}

\subsection{Overview}

The Search application needs to read in or receive metadata about
BBC content from a variety of sources and then needs to serve
a diverse set of use cases.\cite{fenning2014applicability} The
need to integrate against different data sources and funnel
that information into a common set of search indexes can
typically be seen as an enterprise integration
problem.

Some of the data are fairly flat, textual content such as articles and some
information -- such as programmes -- is very structured (e.g. programmes,
series, episodes, times of broadcast, channel on which they are broadcast,
what times they are available to watch on iPlayer, what kinds of devices
are permitted by rights to watch them on iPlayer). There are challenges
in providing a consistent and meaningful user experience across such
a heterogeneous mix of content.

Some uniformity of the data is achieved by a single, common
API through which all content is fed into the Search application. However,
some integration work needs to be
done by either the owners of the content to send content to the API
or the developers of the Search application, who can integrate
existing feeds via an adapter pattern.

Some pages on the BBC website no
longer have a dedicated team maintaining them and can sometimes only
be discovered via a web crawl as an external search engine would do. A lot
of metadata about content also lives in a diverse range of databases and
content management systems, which may or may not be easily retrievable
via the public web.

\subsection{Current Implementation}
\label{sec:current-impl}

\begin{sidewaysfigure}
  \begin{center}
\begin{dot2tex}[dot,scale=0.7]
digraph ingest {
  rankdir=TB;

  subgraph cluster_0 {
    label = "BBC Websites";
    Programmes [shape=rect]
    News [shape=rect]
    Newsround [shape=rect]
    Newsbeat [shape=rect]
    Sport [shape=rect]
    MarketData [shape=rect,label="Market Data"]
    WorldService [shape=rect,label="World Service"]
    BBCFood [shape=rect,label="BBC Food"]
    ProgrammesWebsites [shape=rect,label="Programmes Microsites"]
    LegacyWebsites [shape=rect,label="Legacy Websites"]
  }

  subgraph cluster_1 {
    label = "Search Application";
    FileBasedQueues [label="Message Queues"]
    IngestAPI [label="Ingest API"]
    SearchIndexes [shape=circle]
    Crawler
    trans2 [label="Transformation"]
    Transformation
  }

  subgraph cluster_2 {
    Dynamite
    CPS
    label = "Back-end Services";
  }

  Programmes -> Dynamite
  Dynamite -> FileBasedQueues

  News -> CPS
  Newsround -> CPS
  Newsbeat -> CPS
  Sport -> CPS  
  CPS -> FileBasedQueues

  MarketData -> FileBasedQueues

  trans3 [label="Transformation"]
  WorldService -> trans3
  trans3 -> IngestAPI
  
  ProgrammesWebsites -> Crawler
  BBCFood -> Crawler
  LegacyWebsites -> Crawler
  Crawler -> trans2
  trans2 -> IngestAPI

  IngestAPI -> SearchIndexes

  FileBasedQueues -> Transformation
  Transformation -> SearchIndexes
}
\end{dot2tex}
  \end{center}
  \caption{Abstract model of indexing process}
  \label{indexing-chain}
\end{sidewaysfigure}

Figure 1 shows an informal view of the flow of information into the
search indexes. Note that some content is transformed
by integration applications written by the Search team whilst some
is pushed via a so-called ``Ingest API''. This API ensures that
all content is normalised to some common format and vocabulary before
it is sent to the indexes. From the perspective of the search application
system, this reduces the complexity of having to map multiple formats
and vocabularies to the common representation.

However, if we consider the holistic BBC online system, that transformation
and integration work still needs to happen. It is arguably more efficient to
push the integration efforts back onto the respective teams and domain experts
in charge of each type of content, but that then puts burden on those teams
to prioritise integrating with the search system over other work planned
for their product. This is especially difficult again for content that no
longer has a team actively maintaining it, which is shown by the need
to transform all content found via the Crawler; there is no clear team to
whom the transformation/integration could be delegated.

It is clear that the search application has the need to normalise vocabularies
to some extent (e.g. is it ``title'', ``headline'' or ``display title''
that I should display as the link text on the results page?) and this requires
some form of transformation. This frames the search application as being a
key system whose primary r\^ole is to break down all the content silos in the
BBC. This aligns with the one of the major motivations of linked data, which
is to break down content silos within organisations or indeed globally.

It seems there needs to be consideration around balancing the amount of
integration work delegated to other teams. From the perspective of teams
whose products produce content, the ideal search system would be
``light touch'' and reasonably passive at indexing their content. This
expectation is strengthened by the long history of public, general-purpose
web search engines such as Google who seem to find websites by themselves
and index them without any input from webmasters. However, it should
not be overlooked that Google does in fact place some burden on webmasters
to increase the crawlability and indexability of their websites. This
burden is welcomed due to the benefits of higher rankings that emerge
after sinking those costs.

Can the same be said that a particular web team
in the BBC would welcome additional integration effort for the search system?
Are there benefits that would justify their integration costs? Perhaps if
these teams have already sunk the costs of optimising their content for
Google, the search system can at least make use of these efforts and only
require additional integration work if there are further benefits?

\section{The Content}
\label{content}

The front end search application currently divides all the searchable
content into so-called ``zones''. Table~\ref{zones-table} explains
each of these categories.

\begin{sidewaystable}
  \centering
\begin{tabular}{| p{5cm} | p{16cm} |}
  \hline
  Zone/Category          & Notes/Comments \\
  \hline
  About the BBC          &  Static pages, blogs, articles. RSS/Atom feeds available. \\
  \hline
  Blogs                  &  Legacy blog content. RSS/Atom available. \\
  \hline
  CBBC                   &  Pages, games, programmes information for Children. Has semantic markup for Facebook's Open Graph. \\
  \hline
  Films                  &  Legacy site for BBC Film Network and current site for BBC Films. Little semantic markup. \\
  \hline
  Food                   &  Recipes features on BBC programmes. No semantic markup despite sister website BBC Good Food website using Schema.org microdata. \\
  \hline
  Gardening              &  Legacy website not updated since 2006. All static HTML content with no semantic markup. \\
  \hline
  Health \& Parenting    &  Websites about health, pregnancy, relationships, etc. Most have been left unchanged for several years and some have in fact been taken down, but entries still exist for the removed pages in the search indexes. \\
  \hline
  History                &  A mixture of legacy sites relating to history and a revamped History home page until the banner of the new ``Knowledge and Learning'' collection of sites. \\
  \hline
  iPlayer                &  On-demand catch-up service for TV and Radio programmes broadcast on the BBC. Videos are traditionally only available for 7 days (this will change to 30 days soon) after broadcast. \\
  \hline
  Learning               &  A vast collection of websites with learning materials for children and adults alike. The content varies from very old pages to maintained ones revamped under the ``Knowledge and Learning'' brand.\\
  \hline
  Local (archive)        &  Legacy pages aimed at different local areas of the UK. Newer content is generally under the remit of BBC News, which has sections for different UK regions and local news therein. \\
  \hline
  Music                  &  Sites about music artists, reviews of albums, BBC orchestras, etc. The artist pages are driven by semantic metadata (e.g. Musicbrainz, DBPedia), but only markup for Facebook's Open Graph occurs in the pages.\\
  \hline
  News                   &  News articles published over several years. The quality of the pages and the markup varies with the age of the content, but newer articles contain some semantic markup. \\
  \hline
  Ouch!                  &  Legacy posts for BBC's blog about disability. From April 2013, blogs became part of BBC News, but links to older posts remain in the indexes. \\
  \hline
  Religion \& Ethics     &  Website under the ``Knowledge and Learning'' giving information on religion, culture and traditions. \\
  \hline
  Science \& Nature      &  Old and new sites including the maintained Nature site, which uses semantic data to generate a lot of the content\cite{raimond2010use}, and the Science site -- another site under the ``Knowledge and Learning'' brand. \\
  \hline
  Sport                  &  Up-to-date articles and information about sport. BBC Sport some similarities with BBC News in terms of articles, but also has some more quantitative data such as match results and fixture dates. \\
  \hline
  Teenagers              &  Legacy ``Switch'' site aimed at teenagers covering music, TV and games. \\
  \hline
  TV \& Radio Programmes &  Programmes microsites for every programme, individual episodes and additional content such as clips and character information. This content is obtained currently by crawling. \\
  \hline
  TV \& Radio Sites      &  Crawled ``home'' pages for BBC TV and Radio services with crawled, legacy pages about BBC programmes before the common platform for programmes microsites was developed.\cite{raimond2010use} \\
  \hline
\end{tabular}
\caption{Categories or ``zones'' of content currently indexed by BBC Search.}
\label{zones-table}
\end{sidewaystable}

Some of these zones or categories are related to major, well-known areas of
BBC online. Some seem to be more esoteric or legacy areas, many of which
are obtained through a web crawler. The presence of ``iPlayer'',
``TV \& Radio Programmes'' and ``TV \& Radio Sites'' is a distinction that
has arisen for technical and historic reasons, which is not likely to be
clear to users.

There is a wealth of legacy content that is likely to be obtainable only by
crawling. Some active sites are syndicated via RSS/Atom, but a subscription-based
feed does not allow indexing a back-fill of content easily.

\subsection{Programmes}
\label{programmes}

Figure~\ref{question-time} depicts semantic metadata found within the home page
of the \emph{Question Time} microsite. The CURIE\cite{birbeck2009curie}
\texttt{bbcprog:b043b2rs} (the prefix \texttt{bbcprog} is used here in place
of \texttt{http://www.bbc.co.uk/programmes/} for convenience)
describes the page itself as visited in the
browser. This home page for Question Time is a destination the search
application might send a user who has indicated in their query that it
is likely a page they are looking to find, e.g. if they had simply
searched for ``question time''.

In this case, this graph of metadata has come not just from microdata
in the home page, but also by merging it with the alternative RDF/XML
representation that appears to contain more detailed information.
It was necessary to have some knowledge of this behaviour of the
programmes microsites -- that they have multiple representations
in different media types -- since the microdata in the HTML view
does not link to the RDF/XML view in any way. Such small bits of
knowledge and business rules about the BBC's own online content
is a way a BBC-specific search product can gather information publicly-available
as linked data, but in a more considered way and have advantages over
public search engines such as Google and Bing.

\begin{sidewaysfigure}
  \begin{center}
    \begin{tikzpicture}[scale=0.35,transform shape]
    \begin{dot2tex}[dot,pgf,codeonly]
      \input{question-time.dot}
    \end{dot2tex}
    \end{tikzpicture}
  \end{center}
  \caption{Semantic metadata found within \emph{Question Time} home page}
  \label{question-time}
\end{sidewaysfigure}

Note the distinction between the identifier for the microsite's
\emph{page} and the identifier for concept of \emph{Question Time}
itself. For the latter, the URI
\texttt{https://www.bbc.co.uk/programmes/b043b2rs\#programme}
is used.\cite{raimond2010use}

This abstract entity is classified as a \emph{Brand} in the
Programmes Ontology\cite{raimond2009bbc} (\texttt{po:Brand})
developed by Raimond et al. in 2009. This and other concepts
from the Programmes Ontology were also adopted into the
Schema.org vocabulary in 2013.\cite{raimond2013schema}
Figure~\ref{question-time} also shows
that Question Time is thus classified as a \emph{Series}
(\texttt{schema:Series}). This could cause confusion given
that there is a Series class also within the Programmes Ontology,
whose equivalent in Schema.org is a \emph{Season}. This emphasises
the importance of firmly understanding what vocabulary is in use
when integrated with a system and having consistent, considered
mappings between vocabularies where needed.

The Question Time brand has associated metadata that might
be useful for a search application to display or index. A title
suitable for displaying is given along with synopses of two
different lengths, which might form part of a snippet. We are also
told that the programme falls within \emph{genres} identified
by \texttt{bbcprog:genres/factual/politics\#genre} and
\texttt{bbcprog:genres/news\#genre}. This URI pattern is
clear enough to a human reader to represent a \emph{Politics}
sub-genre (underneath a \emph{Factual} genre) and a
\emph{News} genre respectively. However, if we wish to make
use of this in any applications, the correct approach with
linked data is to dereference those identifiers to find
metadata we can display or index.

Following the politics sub-genre URL, we can produce the
semantic representation shown in figure~\ref{politics}.
An indexing process that follows certain linked entities
could, for example, index the Question Time home page
against the keywords ``politics'' or index a film's
page against the names of actors that appear therein.
Linked concepts might also drive decorations and controls
on the search results pages themselves or the search
application might even be able to provide faceting, filtering
and aggregation across content that shares common properties.

\begin{sidewaysfigure}
  \begin{center}
    \begin{tikzpicture}[scale=0.6,transform shape]
    \begin{dot2tex}[dot,pgf,codeonly]
      \input{politics.dot}
    \end{dot2tex}
    \end{tikzpicture}
  \end{center}
  \caption{Semantic metadata around the \emph{Politics} programme genre}
  \label{politics}
\end{sidewaysfigure}

Returning to the semantic view of Question Time in figure~\ref{question-time},
we can see that the page also describes another entity representing
an episode \emph{episodes} with identifier \texttt{bbcprog:b006t1q9\#programme}.
There are a far greater
number of episodes linked, but the illustration here is reduced to
just the two for brevity.

One of the episodes has links to its own page and information
about how to watch that episode. The graph in figure~\ref{question-time}
depicts the most recent episode at the time of writing and the
semantic data given tells us quite a lot about it. We can see
that there is a linked video (representing as blank node of
class \texttt{schema:VideoObject}), named ``BBC iPlayer'' with
a URL (\texttt{schema:url}) of \texttt{http://www.bbc.co.uk/iplayer/episode/b043b2rs}.
This tells us that we can currently follow that URL to watch
this episode on BBC iPlayer, which is an example of a useful
call-to-action that might be placed a search results page where
a programme is one of the results returned.

We can also see that the time interval in which this is available
to watch (\texttt{schema:OnDemandEvent}) ends on 8 May, 2015. A
search application might make use of this to prefer or perhaps
only show results for programmes that are within this availability
window. This is the current behaviour for search feature within
the iPlayer website, but all the availability information is
obtained via direct integration with service APIs.

The time and date of the next broadcast of this episode is also shown.
From the graph depicted, we can see there is a
broadcast to occur on Sunday, 11 May 2014 on the BBC Parliament
channel. Again, this is information that can be displayed within
search results that might well answer some queries directly
such as ``When is Question Time next on TV?''. This is similar
to functionality offered by Google where currency conversions,
flight times and other quick answers are displayed at the top
of the results for any query it deems has a straight answer.

Finally, note there are some metadata about the pages themselves
that are repeated within the Open Graph protocol namespace
(prefixed with \texttt{og:}). It is common for BBC pages (and
in fact pages on the web in general) to have embedded Open
Graph metadata to integrate into Facebook's social graph, even
when no other semantic markup or metadata are present. Whilst
the original intention was for Facebook integration, linked
data technologies will allow us to derive information from
these properties beyond their intended purpose.

Overall, there are a lot of metadata obtainable about BBC
programmes via semantic markup available on the public web.
However, there is
still some information that we cannot obtain:

\begin{itemize}
  \item We cannot easily gather a list of all identifiers that might
    be interesting to the search application. Crawling links with
    web crawlers is one approach, but there is no guarantee of finding
    everything and there are difficulties knowing when to get an
    updated version of a page when it changes. A BBC-specific
    search can and arguably should make use of internal integration
    to discover and update programmes information as it is created,
    changed or removed.
  \item BBC iPlayer can display the names of contributors to a programme
    such as the producers, directors, actors, etc. This information
    does not appear to be present in the semantic metadata within
    the programmes microsites.
  \item Linked Data follows an \emph{Open World} assumption,
    which means that the lack of fact being asserted does not imply
    that fact is not true. In our Question Time example above, the
    fact that Question Time is not asserted to be a comedy does not
    strictly imply that it is not a comedy.
\end{itemize}


The open world assumption is not always problematic
if we are positively adding
decoration, calls-to-action, query matching, etc. based on
facts we do have and just allow things to be missing when
the data are insufficient. Problems are introduced, however,
if we start to make assumptions, for example, that the
OnDemandEvent time interval being in the past means the
programme is definitely not available to watch in iPlayer.

In this example, missing off the link to iPlayer might merely
inconvenience a user who has to click around a bit more to
find the iPlayer link. If, however, the search application
chose not to respond with the programme at all under the
impression it is not available to watch, then there is
arguably a more broken user experience.

In some situations it may be preferable to integrate against
a service or data source to get a definitive and \emph{complete}
answer, i.e. one with which we can make closed world assumptions.
In other words, if the data source does not tell us a programme
is available right now to watch on BBC iPlayer, then we can
strictly and validly infer that it is not available to watch.

The trade-offs between open and closed world assumptions need to
be kept in mind for any information architecture around the
search results pages and for any domain modelling for the
search indexes. Care should be taken around any design element
or feature that is affected by the lack of information. Ideally,
all behaviour would follow a \emph{progressive enhancement} model
where additional information can only improve the system's
behaviour
and the omission of details or data leads to a \emph{degraded}
rather than \emph{broken} experience.

The programmes data set is normally the most structured and
complex to work with, but is one of the leading uses of
semantic web technologies on the BBC website as a whole. In
the rest of this section, I will briefly touch on any different
challenges from other data sets.

\subsection{News}

The semantic metadata that can be found in a BBC News article
page is notably less complex than a programmes microsite, as
can be seen in figure~\ref{keepod}.

\begin{sidewaysfigure}
  \begin{center}
    \begin{tikzpicture}[scale=0.5,transform shape]
    \begin{dot2tex}[dot,pgf,codeonly]
      \input{keepod.dot}
    \end{dot2tex}
    \end{tikzpicture}
  \end{center}
  \caption{Metadata found within a BBC News article page}
  \label{keepod}
\end{sidewaysfigure}

As discussed in section~\ref{programmes}, BBC News also makes
use of Facebook's Open Graph metadata and a search application
could derive information about this article from those properties.

There is also a second vocabulary in use, namely rNews -- an RDFa
vocabulary developed by the International Press Telecommunications Council
(IPTC) to annotate online news content.

Note that \texttt{rnews:headline} and \texttt{og:title} agree on
the title/headline for the article, but \texttt{og:image} and
\texttt{rnews:thumbnailUrl} point to two different URLs for
an image to accompany the article. In this case, \texttt{og:image}
just points to a BBC News logo image and \texttt{rnews:thumbnailUrl}
shows an image related to the article, albeit only 66 pixels wide
by 49 pixels high.

So, if we wish to create a search application that shows thumbnails next
to results, which image is better to use? Can we create general-case
rules that apply more ``trust'' to one image property over another?

Also, a larger problem is that a semantic extraction on this page
has not actually given us the article's text content in any way. The
current BBC Search application uses all text within an article as
potential keywords so that a user can search for phrases that appear
anywhere within an article in theory. If we are to extract that text
for indexing, we encounter all the usual pitfalls of non-semantic
web crawling and indexing -- e.g. it is not always clear where
in the HTML the article's ``body'' is (we might accidentally index
things like the navigation menu) and the crawler needs updating
if the HTML layout of the page changes.

One significant gap in the semantic markup of BBC News articles is
the lack of relating the content of the articles to notable entities.
For instance, any mention of people or places could be captured
to provide aggregations across those concepts or at least make it
easier to find an article by typing the names of concepts and things
mentioned therein.

Annotations of concepts in an article is best done by journalists
and editors, but can we automate some of the process, at least
to suggest annotations to use? One approach is to make use of
DBPedia Spotlight to extract concepts that may be mentioned in
an article.

\begin{sidewaysfigure}
  \begin{center}
    \begin{tikzpicture}[scale=0.41,transform shape]
    \begin{dot2tex}[dot,pgf,codeonly]
      \input{keepod-tagged.dot}
    \end{dot2tex}
    \end{tikzpicture}
  \end{center}
  \caption{BBC News article page's metadata enriched using DBPedia Spotlight}
  \label{keepod-tagged}
\end{sidewaysfigure}

A na\"ive first-pass use of DBPedia Spotlight could enrich
our article's semantic graph to that depicted in figure~\ref{keepod-tagged}.
Arbitrary values of 10 and 0.25 were used for the \emph{support}
and \emph{confidence} parameters respectively. These values were
chosen based on the their returning a small handful of tags, but
a more considered approach might dynamically adjust these values
optimised for desired behaviour of the search application.

Automatic tagging with systems such as DBPedia Spotlight is a good
example of an approach that \emph{can} be taken to enrich
the metadata, but it is not clear from the analysis so far as to
whether the search application \emph{should} enrich in this way
nor what benefits it brings.

One remaining piece of information that is still lacking is
the News category in which the article falls. The BBC News
website has business rules around categorising all news
content -- e.g. \emph{Technology}, \emph{Scotland}, \emph{Education}
-- but this categorisation seems lacking from the semantic markup. The
current BBC Search application decorates all news results with
their respective news category, so an indexing chain using
the linked data within the news pages alone would need need
to enrich with that information.

It should not be forgotten that BBC News content dates back to the
1990s and semantic metadata that can be extracted is going
to vary greatly in quality. The search application is sometimes
the only way to navigate to such historic content and any indexing
work needs to be accepting of this variation over the years.

\subsection{Sport}

The BBC Sport website shares a lot with BBC News in that a large
amount of the content is article-based. However, the sport domain
has a lot of structured data beneath the surface: atheletes,
teams, competitions, disciplines, events, etc.

An ontology was defined to drive some automatically aggregated
pages during the 2012 Olympics\cite{rayfield2011bbc}, but reading
semantic data from a typical article yields a picture very similar
to the News story example in figure~\ref{keepod}.

There is a lot of potential for structured data to create an
intelligent search experience whereby, for example, searching
for a football team by name would suggest links to the last
match results, dates of when they are next playing or even
summarise the current league table for the relevant football
league. This is something for much later than the proposed
road map in this paper, but it can be hoped that improvements
from using linked data in other areas of the BBC can inspire
some interesting collaboration with the BBC Sport teams.

\subsection{Knowledge and Learning}

The ``Knowledge and Learning'' brand is an attempt to create
a consistent and cohesive picture of what was a discrete
collection of sites. Topics search as school curricula, revision
guides, science, religion, food and history have structured
domain models behind them, but there are limited semantic data
within the pages themselves. This means the best option for
the current search application is to run a web crawler across
these sites, but there are some efforts to create relevant
ontologies to drive these sites.

One example is the the \emph{Education} website that has
a distinct page for every school stage, school subject
and topic within a subject for all the the UK. These pages
are driven off a domain model and ontology\cite{bbc2013curriculum}
behind the scenes
(i.e. it's not present within the HTML pages), but that
model has been published under a free licence.
\cite{bbc2014curriculum} This freely-available data set
effectively acts as site map of the Education website.

\subsection{Legacy Sites}

With websites that are no longer maintained, there is little choice
but to run web crawlers over these pages. Further complication
comes from the fact that certain pages may be taken down
altogether without the search application necessarily being
notified, so the crawler has to periodically check on
pages already indexed to confirm they are still there.
