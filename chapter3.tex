\chapter{Analysis}

In this chapter, I will give a deeper analysis of the four
components of the search application identified in
chapter~\ref{intro} and start to highlight areas where linked data
approaches or semantic web technologies could have a r\^ole to play.

\section{Indexing Content}

\subsection{Overview}

The Search application needs to read in or receive metadata about
BBC content from a variety of sources and then needs to serve
a diverse set of use cases.\cite{fenning2014applicability} The
need to integrate against different data sources and funnel
that information into a common set of search indexes can
typically be seen as an enterprise integration
problem.

Some of the data are fairly flat, textual content such as articles and some
information -- such as programmes -- is very structured (e.g. programmes,
series, episodes, times of broadcast, channel on which they are broadcast,
what times they are available to watch on iPlayer, what kinds of devices
are permitted by rights to watch them on iPlayer). There are challenges
in providing a consistent and meaningful user experience across such
a heterogeneous mix of content.

We can achieve some uniformity of the data by building a single, common
API through which all content is fed into the Search application. However,
some integration work needs to be
done by either the owners of the content to send content to the API
or the developers of the Search application, who can integrate
existing feeds via an adapter pattern.

Some pages on the BBC website no
longer have a dedicated team maintaining them and can sometimes only
be discovered via a web crawl as an external search engine would do. A lot
of metadata about content also lives in a diverse range of databases and
content management systems, which may or may not be easily retrievable
via the public web.

\subsection{Current Implementation}

\begin{dot2tex}[dot,options=-tmath]
digraph ingest {
  rankdir=LR;

  Programmes -> Dynamite
  Dynamite -> FileBasedQueues

  News -> CPS
  Newsround -> CPS
  Newsbeat -> CPS
  Sport -> CPS  
  CPS -> FileBasedQueues

  MarketData -> FileBasedQueues

  trans3 [label="Transformation"]
  WorldService -> trans3
  trans3 -> IngestAPI
  
  ProgrammesWebsites -> Crawler
  BBCFood -> Crawler
  LegacyWebsites -> Crawler
  trans2 [label="Transformation"]
  Crawler -> trans2
  trans2 -> IngestAPI

  IngestAPI -> SearchIndexes

  FileBasedQueues -> Transformation
  Transformation -> SearchIndexes
}
\end{dot2tex}

Figure 1 shows an informal view of the flow of information into the
search indexes. Note that some content is transformed
by integration applications written by the Search team whilst some
is pushed via a so-called ``Ingest API''. This API ensures that
all content is normalised to some common format and vocabulary before
it is sent to the indexes. From the perspective of the search application
system, this reduces the complexity of having to map multiple formats
and vocabularies to the common representation.

However, if we consider the holistic BBC online system, that transformation
and integration work still needs to happen. It is arguably more efficient to
push the integration efforts back onto the respective teams and domain experts
in charge of each type of content, but that then puts burden on those teams
to prioritise integrating with the search system over other work planned
for their product. This is especially difficult again for content that no
longer has a team actively maintaining it, which is shown by the need
to transform all content found via the Crawler; there is no clear team to
whom the transformation/integration could be delegated.

It is clear that the search application has the need to normalise vocabularies
to some extend (e.g. is it ``title'', ``headline'' or ``display title''
that I should display as the link text on the results page?) and this requires
some form of transformation. This frames the search application as being a
key system whose primary r\^ole is to break down all the content silos in the
BBC, which was noted as a motivation for Linked Data in chapter 2.

There is also an apparent requirement that we are cautious of delegating
too much integration work to other teams. From the perspective of teams
whose products produce content, the ideal search system would be
``light touch'' and reasonably passive at indexing their content. This
expectation is strengthened by the long history of public, general-purpose
web search engines such as Google who seem to find websites by themselves
and index them without any input from webmasters. However, it should
not be overlooked that Google does in fact place some burden on webmasters
to increase the crawlability and indexability of their websites. This
burden is welcomed due to the benefits of higher rankings that emerge
after sinking those costs.

Can the same be said that a particular web team
in the BBC would welcome additional integration effort for the search system?
Are there benefits that would justify their integration costs? Perhaps if
these teams have already sunk the costs of optimising their content for
Google, the search system can at least make use of these efforts and only
require additional integration work if there are further benefits?

\subsection{The Content}
\label{content}

The front end search application currently divides all the searchable
content into so-called ``zones''. Table~\ref{zones-table}

%TODO - is this all of them?
\begin{table}[h]
\begin{tabular}{| l | l |}
  Zone/Category          & Notes/Comments \\
  \hline
  About the BBC          &  Static pages, blogs, articles. RSS/Atom feeds available. \\
  Blogs                  &  Legacy blog content. RSS/Atom available. \\
  CBBC                   &  Pages, games, programmes information for Children. Some semantic markup exists for Facebook's Open Graph. \\
  Films                  &  Legacy site for BBC Film Network and current site for BBC Films. Little semantic markup. \\
  Food                   &  Recipes features on BBC programmes. No semantic markup despite sister website BBC Good Food website using Schema.org microdata. \\
  Gardening              &  Legacy website not updated since 2006. All static HTML content with no semantic markup. \\
  Health \& Parenting    &  \\
  History                &  \\
  iPlayer                &  \\
  Learning               &  \\
  Local (archive)        &  \\
  Music                  &  \\
  News                   &  \\
  Ouch!                  &  \\
  Religion \& Ethics     &  \\
  Science \& Nature      &  \\
  Sport                  &  \\
  Teenagers              &  \\
  TV \& Radio Programmes &  \\
  TV \& Radio Sites      &  \\
\end{tabular}
\caption{Categories or ``zones'' of content currently indexed by BBC Search.}
\label{zones-table}
\end{table}

Some of these zones or categories are related to major, well-known areas of
BBC online. Some seem to be more esoteric or legacy areas, many of which
are obtained through a web crawler. The presence of ``iPlayer'',
``TV \& Radio Programmes'' and ``TV \& Radio Sites'' is a distinction that
has arisen for technical and historic reasons, which is not likely to be
clear to users.

There is a wealth of legacy content that is likely to be obtainable only by
crawling. Some active sites are syndicated via RSS/Atom, but a subscription-based
feed does not allow indexing a backfill of content easily.

\section{Search Results Page}

The default results page currently returned for a query breaks the individual
results into the zones or categories mentioned in section~\ref{content}. A
navigation menu on the left allows users to drill down into the results just
for that category.

\section{Editor's Choice}

Each and every link promoted within this feature is hand-chosen by editorial
staff and much manual effort is needed to keep promoting new concepts
as they appear and to remove pages as they get superseded. It is hoped
in this proposal we can reduce some of that manual curation through the use
of linked data and demonstrate ways editorial staff can focus on curating
their own linked data such that Editor's Choice could evolve into a more
generalised and automated subsystem. A metadata-driven curation system
could use common patterns in the data to promote orders of magnitude more
top links with a fraction of the current effort.
However, there would be a need to get a list of the known static pages
as they will not appear in the RSS/Atom feeds. Some integration with the
underlying CMS may be necessary or we would have to rely on web crawling
to find these pages.

\section{Search Suggest}

The list of suggestions that might surface in this system are
currently chosen and maintained by hand by editorial staff. If a new
concept breaks into the news (such as a notable person becoming notable
for the first time), then a person must enter that term into the system.
Similarly, if something should not be suggested to users any longer
(perhaps for legal reasons or simply because it is no longer a relevant
concept), then it must be amended or removed manually.

This manual maintenance of the suggestion list also leads to a
disconnect between searchable content and suggested queries that might
appear as users type. There is an onus on editorial staff to ensure
that content will be found when a suggested query is used. It might
serve the website users better to find mechanisms by which to generate
a useful set of suggestions from the corpus of content as it is
indexed. The heterogenous nature of BBC content turns this into
further integration work where different business rules are likely required
to generate suggestions from different types of content.
