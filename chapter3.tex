\chapter{Analysis}

In this chapter, I will give a deeper analysis of the four
components of the search application identified in
chapter~\ref{intro} and start to highlight areas where linked data
approaches or semantic web technologies could have a r\^ole to play.

\section{Indexing Content}

\subsection{Overview}

The Search application needs to read in or receive metadata about
BBC content from a variety of sources and then needs to serve
a diverse set of use cases.\cite{fenning2014applicability} The
need to integrate against different data sources and funnel
that information into a common set of search indexes can
typically be seen as an enterprise integration
problem.

Some of the data are fairly flat, textual content such as articles and some
information -- such as programmes -- is very structured (e.g. programmes,
series, episodes, times of broadcast, channel on which they are broadcast,
what times they are available to watch on iPlayer, what kinds of devices
are permitted by rights to watch them on iPlayer). There are challenges
in providing a consistent and meaningful user experience across such
a heterogeneous mix of content.

We can achieve some uniformity of the data by building a single, common
API through which all content is fed into the Search application. However,
some integration work needs to be
done by either the owners of the content to send content to the API
or the developers of the Search application, who can integrate
existing feeds via an adapter pattern.

Some pages on the BBC website no
longer have a dedicated team maintaining them and can sometimes only
be discovered via a web crawl as an external search engine would do. A lot
of metadata about content also lives in a diverse range of databases and
content management systems, which may or may not be easily retrievable
via the public web.

\subsection{Current Implementation}

\begin{dot2tex}[dot,options=-tmath]
digraph ingest {
  rankdir=LR;

  Programmes -> Dynamite
  Dynamite -> FileBasedQueues

  News -> CPS
  Newsround -> CPS
  Newsbeat -> CPS
  Sport -> CPS  
  CPS -> FileBasedQueues

  MarketData -> FileBasedQueues

  trans3 [label="Transformation"]
  WorldService -> trans3
  trans3 -> IngestAPI
  
  ProgrammesWebsites -> Crawler
  BBCFood -> Crawler
  LegacyWebsites -> Crawler
  trans2 [label="Transformation"]
  Crawler -> trans2
  trans2 -> IngestAPI

  IngestAPI -> SearchIndexes

  FileBasedQueues -> Transformation
  Transformation -> SearchIndexes
}
\end{dot2tex}

Figure 1 shows an informal view of the flow of information into the
search indexes. Note that some content is transformed
by integration applications written by the Search team whilst some
is pushed via a so-called ``Ingest API''. This API ensures that
all content is normalised to some common format and vocabulary before
it is sent to the indexes. From the perspective of the search application
system, this reduces the complexity of having to map multiple formats
and vocabularies to the common representation.

However, if we consider the holistic BBC online system, that transformation
and integration work still needs to happen. It is arguably more efficient to
push the integration efforts back onto the respective teams and domain experts
in charge of each type of content, but that then puts burden on those teams
to prioritise integrating with the search system over other work planned
for their product. This is especially difficult again for content that no
longer has a team actively maintaining it, which is shown by the need
to transform all content found via the Crawler; there is no clear team to
whom the transformation/integration could be delegated.

It is clear that the search application has the need to normalise vocabularies
to some extend (e.g. is it ``title'', ``headline'' or ``display title''
that I should display as the link text on the results page?) and this requires
some form of transformation. This frames the search application as being a
key system whose primary r\^ole is to break down all the content silos in the
BBC, which was noted as a motivation for Linked Data in chapter 2.

There is also an apparent requirement that we are cautious of delegating
too much integration work to other teams. From the perspective of teams
whose products produce content, the ideal search system would be
``light touch'' and reasonably passive at indexing their content. This
expectation is strengthened by the long history of public, general-purpose
web search engines such as Google who seem to find websites by themselves
and index them without any input from webmasters. However, it should
not be overlooked that Google does in fact place some burden on webmasters
to increase the crawlability and indexability of their websites. This
burden is welcomed due to the benefits of higher rankings that emerge
after sinking those costs.

Can the same be said that a particular web team
in the BBC would welcome additional integration effort for the search system?
Are there benefits that would justify their integration costs? Perhaps if
these teams have already sunk the costs of optimising their content for
Google, the search system can at least make use of these efforts and only
require additional integration work if there are further benefits?

\subsection{The Content}

The front end search application currently divides all the searchable
content into so-called ``zones'',

\section{Search Results Page}

\section{Editor's Choice}

\section{Search Suggest}
