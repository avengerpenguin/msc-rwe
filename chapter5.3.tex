\section{Road Map}

The BBC Search application differs somewhat from many other sites in that
the vast majority of the effort, complexity and codebase lives well
below the HTML/CSS/Javascript surface. The web application side follows
a typical stack comprising Javascript/CSS interface, server-side code/templates
to render the pages and a data layer (the search indexes). However, it has been
shown so far that there are orders of magnitude more complexity upstream
of this data layer, but I hope I have shown that linked data technologies
will reduce some of the unnecessary complexity at least.

This has implicates for planning features, stories, tasks, etc. within
an Agile methodology as the first story to display the most basically
functionality on a web page can easily lead to a wealth of engineering
behind the scenes. In this case, there are a lot of systems already in
place, so this road map will attempt to keep features as iterative as possible.

\subsection{Fully-Response Search Results Page}

Whilst there are several improvements for end users due to better
curation of suggested links and rich snippets, minor improvements
might best be accompanied with more aesthetic changes to increase
their impact. Also, it is good practice in \emph{Behaviour Driven Development}
(BDD) to frame changes in terms of desired behavioural changes from
the perspective of users of the system. To achieve this, the first
feature to be addressed will be rebuilding a responsive results
page in parallel to the existing one.

When the new, responsive page is viable for public use, a switchover
can happen to move people away from the old page. However, this
switchover does not need to happen until some of the later features
have been developed. Whether the new, responsive page is in
use or not, all features will be defined around improvements to
it.

David Marland stated that the ``core principle in creating a potentially
enormous website that will last forever is to get the information
architecture right in the first place'' in discussion around
building the responsive programmes microsites.\cite{marland2014responsive}
Given that
search is also a potentially enormous website driven by of the
order of millions of data items, it is fitting to map out the
information architecture in this first feature and use that to
drive any domain modelling and vocabularies the search application
needs to use.

Thus this first feature should include some principles of
\emph{Domain-driven Design}\cite{evans2004domain}
in that the search team should identify
a domain model of the search page, the results that appear therein
and a common vocabulary that can describe those items even if
they originated in different systems with differing vocabularies.

\begin{figure}
  \begin{center}
    \begin{dot2tex}[dot,pgf,scale=0.41]
      \input{basic-ontology.dot}
    \end{dot2tex}
  \end{center}
  \caption{Basic ontology modelling the traditional ``title, link, description'' search result}
  \label{fig:initial-ontology}
\end{figure}

A common model across differing BBC content items has been
attempted\cite{fenning2014applicability} at a high level, but
for first iterations of this feature, a simplistic model
such as that shown in figure~\ref{fig:initial-onotology}
would suffice.

There are modelling decisions still to be
made around whether to use this new vocabulary exclusively
or attempt to follow an existing one such as Schema.org. The
World Wide Web Consortium (W3C) usually recommends the practice
of \emph{extending} existing vocabularies over coining new
ones, but the linked data model always allows us to publish
mappings to existing vocabularies. However, small, bespoke
ontologies are arguably easier to define, control and iterate
in an agile manner and mappings to existing vocabularies
can be published later.

For the rest of this implementation plan, it is assumed
the custom ontology described will be used as replacing it
with equivalent properties from an existing ontology
is not a difficult task at the point of implementation.

Once the model is decided,
the ``Results Page'' subteam can thus focus on initiating
several iterations of user experience design, user testing
and development of the results page and it is expected this
feature will continue to develop in the background as
other features are worked on by other subteams. It is likely
individual stories might add to the domain model even
to make this simple, responsive rebuild of the results page.

The front end application can continue to search the
existing search indexes independant of any work being done
to improve the ingest chain. The importance of developing
this feature first is to ensure the information architecture
and domain models are defined up front. After this point,
the other subteams can proceed with the features below
while the results page developers keep iterating on
a user interaction that evolves through user testing and
feedback.

\subsection{Indexing Programmes Microsites}

This feature involves replacing all programmes entries in
the search indexes with linked data sent through the proposed
indexing chain. The effect for the front end application
will be to unify the three separate categories for
programmes content mentioned in section~\ref{sec:search-results-page}.

In the initial iteration, this will take the form of a single
``Programmes'' category that simply links to microsites
for different programme entities. Direct linking to iPlayer
will be lost temporarily, but restored in the next
feature. Note that users will always be able to reach
iPlayer via programmes microsites.

The ``Ingest'' subteam needs to ensure the Ingest API can
accept content meeting the basic vocabulary outline in
figure~\ref{fig:initial-onotologu}, validating ones
that do not contain the basic fields and evolve the API. If
the results page developers choose to alter the domain
model based on design iterations and user feedback, then
these changes can propogate back as requirements for the
content ingest.

The team should also evaluate any relevant technologies for the
search indexes and ensure they are searchable by the
developers working on the results page.

Elasticsearch is a standalone search server based on Apache Lucene
that can store, index and retrieve schemaless JSON structures.
The indexing chain is to deal with RDF graphs, for which JSON-LD
is a powerful JSON serialisation that can be sent to Elasticsearch
without any transformation.
A simple prototype of an ingest function written in Python
that receives JSON-LD is shown in listing~\ref{lst:ingest}.

\begin{lstlisting}[language=Python]
from elasticsearch import Elasticsearch
import json

def ingest_jsonld(jsonld_string):
  body = {'jsonld': json.loads(jsonld_string)}
  ElasticSearch().index(
    index='search', body=body, doc_type='item')
\label{lst:ingest}
\end{lstlisting}

The focus for the ingest work should then be around setting
up mappings in Elasticsearch (if the prototype proves promising)
to arrange indexes to enable sensible retrieval behaviour.

The \emph{Discovery} subteam will need to investigate
suitable mechanisms for notifying when certain programmes
objects have changed. There are asynchronous, message-based
systems in place witin the BBC specifically around
programmes change notifications and the engineered solution
is likely to make use of that.

Alongside this work, those responsible for enrichment and
inference will need to define (and apply) the rules for
mapping the vocabularies used by programmes microsites
to the ontology and vocabulary used in the search application.
Such rules will extend the form introduced in
section~\ref{sec:ld-indexing}.

\subsection{Linking to iPlayer}

In this feature, direct links to iPlayer are added back into
search results for programmes.

One difficulty is around
the temporal nature of programmes information. The graph
shown back in figure~\ref{question-time} shows
the time of the next broadcast, but that information will
soon expire and the search system might be interested in
reindexing that programme when the nature of its availability
on BBC iPlayer changes.

It is thus likely that the Discovery subteam working on the
URLNotification component will need to develop components
that query an archive of the linked data ingested to look
for items that may need reingested. They may need to
place requirements on the Ingest team to store an archive
of the linked data within a \emph{triple store} or other
data store (note that Elasticsearch has been known to be
suitable to act as both a search service and a NoSQL-style
document store in one). It is then a decision for teh
engineers as to whether complex SPARQL queries can mine
for programmes (or indeed any) content that has passed
a significant point of time at which the information becomes
out of date or whether to augment the simple ontology
in figure~\ref{fig:basic-ontology} with the concept of
a search result's information \emph{expiring} as shown in
figure~\ref{fig:expiring-ontology}.

\begin{figure}
  \begin{center}
    \begin{dot2tex}[dot,pgf,scale=0.41]
      \input{expiring-ontology.dot}
    \end{dot2tex}
  \end{center}
  \caption{Concept of a potential search result's information expiring has been added}
  \label{fig:expiring-ontology}
\end{figure}

This expiry concept is not necessarily that the content item
should no longer appear in the search indexes, but that the
information should be considered stale and refetching the data
is recommended. This is not too far removed from Google
recrawling pages based on some assessment of how often the page
should be checked, but here we can use the semantic metadata
available publicly (and indeed internally) to get a more
precise idea of a good expiry date.

At this stage, it could
be proposed to send a notification to the URLNotification
component a short time after the date and time within the
value of \emph{any}
\texttt{http://schema.org/endDate} property has passed. A more
suitable strategy might emerge as the individual user stories
within this feature are planned out by the development team.

\subsection{Automatic Curation of Programmes Microsites}

The current Editor's Choice feature links to the top-level
brand microsite (e.g. the home page for \emph{Question Time})
for any programme, but generally does not link to the pages
for specific series/seasons nor individual episodes. This gives
rise to that idea that something modelled as a potential
search result in our domain model \emph{may} be a recommended
result, but not all results have this property. This can
lead to the next iteration of the domain model/ontology in
figure~\ref{fig:recommended-ontology}, which introduces a
property that indicates when a \texttt{:SearchResult}
can be said to represent a recommended page for any concept.
For instance, the URI for Question Time as a concept
will have the recommended page of the microsite about that
programme.

\begin{figure}
  \begin{center}
    \begin{dot2tex}[dot,pgf,scale=0.41]
      \input{recommended-ontology.dot}
    \end{dot2tex}
  \end{center}
  \caption{Ontology augmented with the concept of a recommended link for an entity}
  \label{fig:recommended-ontology}
\end{figure}

The Ingest subteam should then look at maintaining a second
index of just the names/titles of potential search results
that are marked as ones that might be recommended. The
results page front end subteam can then focus on being
able to query both this new index and the index of documents
so as to render Editor's Choice and organic search results
links respectively on a single page.

The API through which the front end web application queries for
these recommended links can not only expose these recommendations
as linked data, but also allow an administrative application
to create, maintain and remove recommendations should there
be requirements from editorial staff to be able to do so. In this
case, we should revisit the domain model (i.e. iterate it further)
to distinguish between, say, \texttt{:automaticRecommendedLink}
which prompts editorial staff to create a corresponding
\texttt{:approvedRecommendedLink}. This reflects the typical
workflows in editorial web tools that traditionally store unmoderated,
approved, etc. links, but instead encourages through the use of RDF
to publish this as linked data by default. This again follows the
principles of linked enterprise data laid out in chapter~\ref{stateoftheart}.

The developers working on inference should gather requirements
for inferring recommendations from the upstream RDF graphs,
which in the case of programmes apparently follows from simply
creating a recommendation for any programme brand to its
respective ``home page'' or microsite.

\subsection{Improve Results for Crawled Sites}

The next feature should address removing configuration for
some actively-maintained BBC sites -- such as CBBC and
Knowledge and Learning -- from the existing web crawler
application and instead push discovered URLs through the new
RDF-based indexing chain. The developers working on discovery
should look to push notifications of new, changed and removed
pages to the URLNotification service, even if they originate
from a web crawler.

The enrichment components could be extended to fetch additional
metadata by integrating against backend services that drive
these sites. Investigations will need to be made around this,
but any new information that can be added will be an improvement
over the current way the sites are simply web crawled and scraped.

It is not expected that the domain model should need to change
significantly, so the ingest and results page applications may not
need specific changes, but it is expected that the presentation
of, e.g. the titles, in the search results for these sites will look
cleaner by properly ascertaining the metadata as opposed to relying
on web scraping to takes things out of HTML tags (e.g. the \texttt{<title>}
tag.

This feature is intentionally not specified with as much detail
as the previous features as agile methodologies usually encourage
making most decisions when closer to implementation of the feature, at
which point there may be a lot more information available.

\subsection{Showing Next Broadcast for a Programme}

This is another feature that is not over-specified, but it does
clearly follow from the ability to add BBC iPlayer links to
programmes search results that it should be possible to decorate
those results further with a quick indication of when that
programme is next broadcast on a TV or radio service.

The information should already be present in the RDF graphs being indexed,
so this work is theoretically just focused around those developers
working on the search results web application to generate such
decoration in the presentation layers based on information that is
already present in the search indexes.
